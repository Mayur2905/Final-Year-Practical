{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8AkGzUZXjaP",
        "outputId": "d65df7c3-a905-4438-87cf-d8a3da37ef42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "# Load the dataset\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Preprocess the dataset\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n"
      ],
      "metadata": {
        "id": "r7InWQRFXmZv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Build the DNN model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=16, activation='relu', input_dim=10000))\n",
        "model.add(Dense(units=16, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "lwAeu02RXoHW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=512)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVjpMIBvXrPT",
        "outputId": "08701a74-9989-439c-8b08-abace1a6b703"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "49/49 [==============================] - 3s 39ms/step - loss: 0.5532 - accuracy: 0.7973\n",
            "Epoch 2/10\n",
            "49/49 [==============================] - 1s 28ms/step - loss: 0.3431 - accuracy: 0.8848\n",
            "Epoch 3/10\n",
            "49/49 [==============================] - 2s 34ms/step - loss: 0.2553 - accuracy: 0.9093\n",
            "Epoch 4/10\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.2134 - accuracy: 0.9222\n",
            "Epoch 5/10\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1845 - accuracy: 0.9350\n",
            "Epoch 6/10\n",
            "49/49 [==============================] - 2s 34ms/step - loss: 0.1654 - accuracy: 0.9414\n",
            "Epoch 7/10\n",
            "49/49 [==============================] - 1s 28ms/step - loss: 0.1465 - accuracy: 0.9498\n",
            "Epoch 8/10\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.1330 - accuracy: 0.9548\n",
            "Epoch 9/10\n",
            "49/49 [==============================] - 1s 28ms/step - loss: 0.1198 - accuracy: 0.9600\n",
            "Epoch 10/10\n",
            "49/49 [==============================] - 2s 31ms/step - loss: 0.1087 - accuracy: 0.9654\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f197f12f5e0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCt2nwfwXs-q",
        "outputId": "2e70e11e-013a-4157-f3a1-179ed280fe58"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 2s 3ms/step - loss: 0.3580 - accuracy: 0.8739\n",
            "Test Loss: 0.3580073118209839\n",
            "Test Accuracy: 0.8738800287246704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# Convert the integer sequences to text\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
        "\n",
        "# Define the tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "\n",
        "# Define the function to vectorize sequences\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "# Convert the new review to a sequence of word indices\n",
        "new_review = \"This movie was terrible. I would not recommend it to anyone.\"\n",
        "new_review = tokenizer.texts_to_sequences([new_review])[0]\n",
        "\n",
        "# Vectorize the sequence\n",
        "new_review = vectorize_sequences([new_review])\n",
        "\n",
        "# Make the prediction\n",
        "prediction = model.predict(new_review)\n",
        "print(\"Prediction:\", prediction[0][0])\n"
      ],
      "metadata": {
        "id": "tsGgbyKaXu1F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}